\documentclass[25pt]{tikzposter}

% design based on https://twitter.com/mikemorrison/status/1110191245035479041?lang=en
% and https://github.com/SimonLarsen/tikzpostersdu

\input{preamble.tex}

\begin{document}

% remove offset that would otherwise be fixed by \maketitle
\makeatletter
    \setlength{\TP@blocktop}{.47\textheight}
\makeatother

%% MAIN MESSAGE OF PAPER %%
\colorlet{blockbodybgcolor}{mDarkTeal}
\block{}{
  \color{white}{
    \fontseries{l}\fontsize{130}{100}\selectfont
    % Learning \textbf{physical concepts}  purely \\\\\\
    % from data: We demonstrate how \\\\\\
    % \textbf{generative models} can learn  \\\\\\
    % \textbf{manifolds} of \textbf{differential equations.}
    \textbf{Relevance determination} for \\\\\\ \textbf{explainability}.
    Learning \textbf{manifolds} of \\\\\\ sparse, \textbf{differential
    equations} from \\\\\\ \textbf{partial observations}.
  }
}
\colorlet{blockbodybgcolor}{white}


\begin{columns}
  \column{0.7}
  \block{}{
    %% TITLE OF PAPER %%
    {\Huge\bf Rodent: Relevance determination in ODE \\\\}
    {\Large Niklas Heim, V\'aclav \v Sm\'idl, Tom\'a\v s Pevn\'y} \hfill
    {\Large Czech Technical University, Prague}
  }

  \block{}{
    \begin{center}
    {\huge\bf Learning differential equations}
    \end{center}
    \begin{minipage}{.28\textwidth}
      \innerblock{}{
      \begin{itemize}
        \item We want to find the simplest ODE that describes a dynamical
          system
        \item By simple we mean: minimal order of ODE and minimum number of
          non-zero parameters.
      \end{itemize}
      }
    \end{minipage}
    \hspace{.02\textwidth}
    \begin{minipage}{.28\textwidth}
      \innerblock{}{
        \begin{itemize}
        \item Discover physically meaningful equations
          that might help understand the underlying process.
        \item We can learn manifolds of generating models not only a single process
          % \item Sparsity enforcing generative model
          % \item ODE solver acts as decoder
          % \item Latent variables are physically interpretable
          % \item Capable of learning ODE manifolds
        \end{itemize}
      }
    \end{minipage}

    \begin{tikzfigure}
      \includegraphics[width=.50\textwidth]{rodent.pdf}
    \end{tikzfigure}

    \vspace{1cm}
    \begin{center}
      {\huge\bf Advantages of the relevant ODE identifier}\\
    \end{center}

    \vspace{1cm}
    \begin{minipage}[t]{.28\textwidth}
    \begin{itemize}
      \item \textbf{Explainability.}  The parameters of $\bm z$ are
        decoded through an ODE solver, which gives them physical meaning.
      \item \textbf{Sparsity.} The automatic relevance determination prior on
        $\bm{z}$ encourages the simplest solution with fewest non-zero parameters.
    \end{itemize}
    \end{minipage}
    \hspace{.01\textwidth}
    \begin{minipage}[t]{.28\textwidth}
    \begin{itemize}
      \item \textbf{Partial observations.} Rodent allows learning of an ODE
        without knowledge of all state trajectories. The
        observation operator is assumed to be known.
      \item \textbf{Time series.} The convolutional part of the encoder is
        agnostic to different time series lengths.
    \end{itemize}
    \end{minipage}


    \vspace{1cm}
    \begin{center}
      {\huge\bf Manifold learning \& Reidentification}\\
    \end{center}
    \begin{tikzfigure}
      \includegraphics[width=.50\textwidth]{single_enc_rec.pdf}
    \end{tikzfigure}
    Rodent reconstructions (left column) and latent codes (right column) of a
    harmonic signal in the upper plots. Reidentified reconstruction and
    encodings in the bottom.  The values of the latent codes only change were
    the variable was detected as relevant.  The heatmaps on the right show
    the corresponding encodings for the weights $\bm W$, biases $\bm b$, and
    initial conditions $\bm\xi$. The Rodent reduced
    the latent space to the four truly relevant parameters.
  }

\block{}{
  \begin{minipage}{.05\textwidth}
    \includegraphics[height=\textwidth]{qr-code.pdf}
  \end{minipage}
  \begin{minipage}{.30\textwidth}
    Check out the full paper at \texttt{http://tiny.cc/f6x6gz}\\
    or scan the QR code on the left! \\
    Email \texttt{niklas.heim@aic.fel.cvut.cz}
  \end{minipage}
  \begin{minipage}{.04\textwidth}
    \includegraphics[height=\textwidth]{aic-logo.png}
  \end{minipage}
  \hspace{.03\textwidth}
  \begin{minipage}{.05\textwidth}
    \includegraphics[height=\textwidth]{avast-logo.png}
  \end{minipage}
  \hspace{.01\textwidth}
  \begin{minipage}{.05\textwidth}
    \includegraphics[height=\textwidth]{cvut-logo.jpeg}
  \end{minipage}
  \hspace{.04\textwidth}
}

  \column{0.3}
  \block{Rodent in depth}{
    \textbf{Problem definition:} Assume a time series $\X = [\bm x_1, \bm
    x_2,\ldots, \bm x_K]$ with $\bm x_i \in \mathbb{R}^d$ generated from
    discrete-time, noisy observations
    \begin{equation}
      \bm x_k = H(\bm{\xi}(\Delta t k)) + e_k,
    \end{equation}
    where $k=1\ldots K$, $e_k \sim \N(0,\se^2\mathbf{I})$, and partial obs.
    operator $H$.  The evolution of $\bm\xi(t) \in \mathbb{R}^N$ is governed by
    an ODE:
    \begin{align}
      \label{eq:dyn_sys}
      \frac{\partial \bm{\xi}}{\partial t} 
        = & f(\bm{\theta}, t) \approx \bm{W}\bm{\xi} + \bm{b}.
    \end{align}
    We aim to learn structure and order of the ODE from a set of
    trajectories $\{\X_i\}_{i=1}^{L}$  generated by the same generative process
    but with  different $\bm\theta_i$ and different $\bm\xi_i(0)$, for each
    trajectory, i.e.
    \begin{align}
      \label{eq:series_x}
      \X_i =& H(\psi(\bm\theta_i,\bm{\xi}_i(0), \bm{t}))+\bm{e},
    \end{align}
    where $\bm{t} = [0, \Delta t, \ldots, K\Delta t]$ and ODE
    solver $\psi$.  Assuming we observe a system with expected order $M$, we choose $N
    \geq M$.\\

    \textbf{Rodent:} By combining the ODE state and its parameters in
    $\z=[\bm\theta,\bm\xi(0)]$ we can define the data likelihood as
    \begin{align}
      \label{eq:decoder}
      p(\x|\z) &= \N(\x|H(\psi(\z)),\sigma_x^2).
    \end{align}
    To determine the structure of the ODE, we employ the ARD prior:
    \begin{align}
      \label{eq:ard_model}
      p(\z) &= \N(\z|0,\text{diag}(\laz^2)) &
      p(\laz) &= 1/\laz.%\Gamma(\laz|\alpha_0, \beta_0)
    \end{align}

    The posterior of $\bm z$ is prescribed by
    \begin{align}
      \label{eq:ard_posterior}
      p(\z|\x) = \N(\z|\phi_\omega(\x), \bm{\sigma}_z^2)
    \end{align}
    where mean $\mz=\phi_\omega(\x)$ is a neural net with parameters
    $\omega$.
    The resulting ELBO:
    {\fontsize{25}{20}
    \begin{equation}
    \begin{aligned}
      \label{eq:elbo_reparam}
      \mathcal{L} &= \sum_{i=1}^n \E{p(z|x)}{\frac{(\x_i - \psi(\phi_\omega(\x_i) + \bm{\sigma}_z \odot \bm{\epsilon}))^2}{2\se^2}}
                  + \frac{nd}{2}\log(\se) \\
                  &+ \sum_{i=1}^n \left(
                      \log\left(\frac{\laz^2}{\sz^2}\right)
                      -m + \frac{\sz^2}{\laz^2} + \frac{\phi_\omega(\x_i)^2}{\laz^2}
                  \right),
    \end{aligned}
    \end{equation}}
    with Gaussian noise $\bm{\epsilon} \sim \N(0,I)$, decoder
    $H(\psi(\bm\theta,\bm\xi(0)))\equiv\phi(\z)$, and $\text{dim}(\z) = m$.\\

    \textbf{Encoder:} The encoder $\phi_\omega$ consists of two parts: (i) A
    dense net that receives only a few steps of the beginning of the time
    series, responsible for predicting $\bm \xi(0)$. (ii) A (CNN) that predicts
    $\bm \theta$.  The CNN averages over the time dimension after the
    convolutions, which makes it possible to use samples of different length.\\

    \textbf{Reidentification:} We sample a batch of
    latent codes from the encoder for each input.  The samples
    are used as starting points for another optimization of the reconstruction
    error $R$
    \begin{equation}
      R = \mathrm{MSE}(\phi(\bm z) - \x),
    \end{equation}
    while keeping all irrelevant parameters fixed.
    On the left, four parameters, namely $W_{13}$,
    $W_{31}$, $\xi_{1}$, and $\xi_{3}$ were found to be relevant, so only
    those are changing during the optimization with respect to $R$.
    This means that we stay in the identified model manifold, but are able to
    extrapolate far beyond the training range.   

    \vspace{1.35cm}
  }
  
\end{columns}

\end{document}
